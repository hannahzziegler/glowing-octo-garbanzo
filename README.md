# Scraping the Maryland State Employees Data Webpages

### April 1 Update

I'm still not as far as I would like to be, but I'm very proud of the progress I made on my bot this week in terms of breaking down different problems and things that I can do with my data. For this week, I pivoted from trying to scrape the Maryland state employees webpages by agency/sub-agency/sub-sub-agency and instead began working to pull from the pages that have each employee listed individually with information. The structure of these pages (faceted by letter with multiple pages per letter) is currently the big road block I am facing (pagination_fixed, chat_gpt_workshop and scrape.py). I'm struggling now to get the code to iterate through each letter first and then iterate over the specific pages associated with that letter as a sub-loop. I'm trying to make the iteration happen now by parsing through the URL in increments of 15 (as each page on the site ends in a variable number that is incremented by 15 to get to the next one. Each page starts with that number as zero, and then the next page moves to offset=15, offset=30 and so on until all of the pages have been found). I asked ChatGPT for help a lot during this troubleshooting process, but it keeps trying to find the next page *links* through parsing <a> tages. I haven't been able to reroute its advice to help out in a useful way that iterates through values in the URL yet.

With that said, I started working on a scraper that scraped into different lists for each letter where there are state employees (see scrape_by_letter). I was able to get all of the "A" values pulled down/etc. but am planning Sunday to troubleshoot why some letters get pulled using this (repetitive, but last resort) strategy, so the code may change a bit between now and when Derek sees this. I think I'll try to store the individual letter scrapes within functions and find a way to add them to a big list of every employee based on that (which will be the one that is monitored and written to the CSV in the final bot process), as things seem to be getting lost in that big scraping shuffle now. 
    (As an aside, the main pitfall with relying on this strategy is the fact that I have hard-coded how many pages/multiples of 15 for each letter should be present for the scraper to continue working. This is far from my favorite, but I figured as an absolute last resort to get *all* of the data down, checking up on the number of pages/seeing if jobs have been added every so often (therefore throwing off the number of pages designated in each specific letter) is still much better than scrolling through thousands of pages consistently trying to pull specific rows.)

The working part of my bot (and the one that is currently set up in the YAML files) is one that just pulls the vacancies from the state employee database. I plan to spend the rest of Saturday figuring out a way to structure this data, as right now it writes to a CSV but I need a refresher on what the most useful way for me to aggregate it within the bot would be. I plan to have some sort of data layout with "[x] vacancies have been filled this week. there are [x] vacancies remaining." At the very least, I am 100% confident I will be able to get this working. I feel alright about being able to get the full scrape of all employees functional, as I at least have a good semblance of how to do it (even if it isn't in the most efficient way yet).

My main objectives for this week are setting up the actual functionality of the bot and making sure that it is well-integrated into its Slack form. But I think this past week was a huge step forward, as last time we had a deadline I couldn't even really figure out the best way to pull the data down. Now, I have a bunch of templates for how to moderate the information depending on what I'm able to troubleshoot over the next few days. My main goal for Sunday/Monday/Tuesday is taking more jabs at having a holistic scraper for all of the employees with some help from Derek/ChatGPT/letting the problem sit for a minute (fingers crossed for a breakthrough!) and then putting some time and energy into making sure the Slackbot is working when it is supposed to/fully integrated in terms of how the message is laid out. 

### Pre-Spring Break Update

A disclaimer up top: I wanted to be way further into understanding how to get the data through deducing the problem to a single agency (in my case here, the state education department), but am headed out of town tonight (Saturday) and wanted to have this done before spring break officially began. I plan to keep toodling with things during the week as time permits.

For this assignment, my main goal was understanding the code of Derek's original scraper for this site, what needs to be changed for my specific purposes of this data that I'm pulling and trying to understand how to scrape the specifics that I need using a particular, single agency that I could scrape.

If nothing else, I think I have a much better understanding of this data and its information architecture through working through different parts of Derek's initial code and working to update/replicate things for just the state education department. What I'm mainly stuck on right now is getting just the

Something I didn't get around to this week (because man, pre-spring break work really seems to pile up) was updating portions of Derek's original scraper that pull down positions that aren't actually real. I think it has something to do with the provisions of the code where we are looking for contact information, such as lines like:
    (if row.find_all('td')[1].text.strip().upper() in ['GENERAL LISTING', 'TTY', 'TOLL FREE', 'MARYLAND TOLL FREE', 'GENERAL ASSISTANCE', 'FAX NUMBER', 'INFORMATION', 'TOLL FREE NUMBER', 'FACSIMILE', 'FACSIMILE NUMBER', 'FASCIMILE NUMBER', 'MAIN NUMBERS', 'MAIN NUMBER', 'GENERAL INFORMATION', 'GENERAL ASSEMBLY INFORMATION']:
                continue)

Those were the main instances where I could see things pulling down positions and pages that weren't actually home to employee information, as things like the "general information", "facsmile number" and other contact categories aren't typically routing to individual positions/jobs/people within the website's data, but instead a general toll number line/something similar to this. This is tricky because the information that doesn't have employee names/etc. in the table is structured the same way on each page as information about given vacancies, so I'm working to find solutions to how we can pull only employee information alone or how to distinguish that based on the HTML that we have to work with.

A main thing I was trying to accomplish this week (and didn't quite achieve) was getting sub-sub-agencies from the data. The scraper Derek had only pulled the agency information and subagency information, but many state employers also have additional pages associated with each subagency. For example, with our education department pages that I was workshopping and troubleshooting this week, there is:
    - A page for all the general agencies in the education department: https://www.doit.state.md.us/phonebook/level2offices.asp?AID=MSDE&offset=0
    - A page for each individual agency (in this example, the superintendant's office) that has employee information: https://www.doit.state.md.us/phonebook/OfficeSub.asp?OID=393
    - A page for each sub-sub agency within the superintendent's office and each other sub-agency that isn't currently getting scraped (in this example, the office of legal counsel from the state superintendent's office): https://www.doit.state.md.us/phonebook/OfficeSub.asp?OID=394

I'm having a hard time reducing these different pages to pull from into their most basic parts without hard-coding things into the scraper that won't be repurposable for other departments in the data. I frankly think I'm dealing with a fair amount of data overload just from the sheer number of pages of data to pull from and have hit a bit of a wall with what I've been trying to accomplish in the waning hours before spring break begins. 

There are also some offices that have links pulled down (such as the division of business services: https://www.doit.state.md.us/phonebook/OfficeSub.asp?OID=8066) that don't list any base employees on the page but do link to sub-agencies that have employees we will need to pull down. On a similar note, there are pages that don't link to any employee information whatsoever.

Something I'm wondering about in terms of troubleshooting/not making this scraper extremely convoluted is if there's a way to scrape from the searches for particularly vacant positions (seen here: https://www.doit.state.md.us/phonebook/IndListing.asp?FirstLetter=vacant&Submit=Search&offset=0) and seeing when things disappear or reappear on that page. Shreya had her scraper do something where it searched stuff in a search bar, so I don't know if that would be an easier approach to this task. Even scraping from the general search bar that has every single employee, their office and department in one place (i.e. https://www.doit.state.md.us/phonebook/AdvSearchResults.asp) seems a bit more feasible than dipping in and out of so many pages within the scraper as it works now/as I had outlined it in my head. I definitely plan to toy around with these possibilities over break and early in the week we come back to see if they're easier to work with, but like I said, I just wanted to get this assignment done before heading out for the week, so I'll leave my rambling about problems and troubleshooting at this. :-)

When it's finished, this bot should run in Slack, ideally, but I'm open to options for whatever will make it work best. The best version of this bot gives alerts each time a state employee vacany has been filled and is able to provide swift and timely alerts when that happens. In a perfect world, it should also able to count up how many vacancies there are, how many have been filled and do basic arithmetic so that people who see the bot can understand any slight trends in how state employee is hiring (based solely on how many positions have been filled/not filled since x date). 