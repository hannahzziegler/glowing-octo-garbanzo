# Scraping the Maryland State Employees Data Webpages

### April 8 Update

Overall, this bot proved to be much harder to create than I initially anticipated. My main goal this week was to get the bot into a workable shape of some sort, even if I knew that it might not reach all of the goals I had originally set out for it to do. This week, I was able to get a Slack message from my bot to share to the channel at appropriate times each day, structured the message so that it would link to the latest data and give the aggregate number of vacancies still present and made sure it sent different messages depending on how the vacancies had change in the last 24 hours. It's been a long and arduous process, but I am very proud of the week-by-week progress that I made toward getting something that is workable and does send a coherent message that is tailored to the data being received.

I decided to bite the bullet on a few of the more ambitious elements of my original proposal because, frankly, I've discovered that this is not good data. Most of the data in this database that we're pulling from is so old that I don't see how it could be helpful in the future for monitoring how Wes Moore fills state employee vacancies to fulfill his campaign promise. The Baltimore Sun and other outlets have routinely reported that there are more than 10,000 state employee vacancies, but according to this website that is supposed to track them, there are less than 400. Most of the information is dated as well. For example, the dean of Merrill College listed in the data is currently shown as the dean from 2006. I originally wanted to pull all of the names of folks who filled vacancies, but since a vast majority of the data has shown itself to be dated at best and inaccurate at worst, I didn't see how it would be a productive use of my time to pull names that may not even be helpful in the long-term reporting process using this bot. Therefore, I simplified my message and the general scope of the project.

By the time these major issues started surfacing, it was too late to restart on a different page, but if anyone is looking to replicate this on the future, they should use the site of Maryland Employee Job Openings (found here: https://www.jobapscloud.com/MD/?Keyword=&Loc=&DeptNumber=&OccList=&JobType=&CollegeDegreeRequired=&TeleworkAvailable=&KeywordFullText=0), which may not be updated but surely, at least, has more than 400 entries as the state tries to address this looming issue. I think it's worth a story in and of itself that the state says they have more than 10,000 openings but there is no publicly available, easy-to-find data that shows these vacancies. This leads to a lack of accountability for Gov. Moore and others who are trying to fill the vacancies in the future.

Beside the annoyingness of trying to pull down all of the data (see April 1 update) and how dated it is, I think the rest of this assignment (aka just curating the Slack message) was the most fulfilling part. My Slack message sends each day and counts up the aggregate number of state employee vacancies by re-reading the generated CSV file for that day, counting its rows and storing them in a variable to use in an f-string. Trying to determine how much information to store in the Slackbot was challenging, but when I decided I would forego sending names to the channel, there were only so many options I had. The bot delivers a message that the vacancies have increased, decreased or not changed since the last update. I considered having the bot not send anything if the data hadn't changed day-by-day, but (in a perfect, hypothetical world where this data is very accurate and actually shows the real vacancies) I think it's just as newsworthy when something doesn't happen as the opposite, especially in a bot that is primarily focused on what our elected leaders/those who serve us in state government are doing to meet our needs.

For this scraper, I store a new data file created each day and appended with the date of the scrape. I also have a master file called "md_employees_total.csv" that is rewritten each day with the most recent data. Regardless of if something changes in data or not, the Slack message each day always links to the day's scraped data so it can be easily accessible to those who look at it. If there has been an increase or decrease in the number of rows in the data, the bot also links to the master file's commit history on Github that shows the side-by-side comparison of added rows/etc. based on when things change on the day-to-day. I opted for this Github strategy because, frankly, I ran out of time to keep tooling with the CSV formats for each day/the master file and also be able to test to make the bot work. Overall, I think this is a good enough way of seeing changes, especially because (within the scope of this data) it's probably going to be pretty rare to see 100s of rows added/deleted on a regular basis, so scrolling to see a single change based on the red and green highlights in Github was the best I could muster for data that isn't quite that helpful in the first place.

Finally, my bot also links back to the (dated) state employees database online that got us into this whole mess. I just wanted to do this so that the users could somehow see all of the state employees, even though I wasn't able to pull down directly from the web pages for each letter and each page within each letter. That sums up the content of my actual Slack message that my bot sends each morning.

I think in the future, my bot could likely run on a weekly or even monthly schedule, but I was hesitant to do that this week right off the bat because I got a bit of a late start on actually getting the scraper going (because I couldn't give up on my pipe dream of scraping every single employee row!) I wanted to make sure the messages would run and make sense more than I wanted to make sure the schedule for sending messages wasn't overwhelming a Slack channel with few updates to give. I think once I have a better understanding of how the data is updated, I'll adjust the scraping schedule to make it every week, but felt daily (at least for this week) would give me the most robust way to workshop strategies to finish the bot and overall deliverable. Also, since the messages are different based on if the CSV file has changed or not, I think I accounted for the fact that the daily scraper and bot may not be providing the most altered data to folks right away.

I also think it would be really cool to find a way to scrape the accurate state job openings page and allow users to interact with the bot to learn more about where vacancies exist. For example, if we actually had all 10,000 vacancies pulled down, I would love to see the functionality in this bot where a user could ask "what state department has the most vacancies and how many does it have?" and receive that answer. That could be achieved using a Datasette app for increased functionality and usability, but I also think it would be cool to find a way to do this in Slack internally so that all members of a channel can see the user interaction and collaborate to extract information out of the bot. If we were still pulling down individual names to include in the bot, it would also be worthwhile to find a way for users to filter the actual employees by department and see the percentage of employees in each department that are filed as vacancies. Once again, I have to reiterate that this would only work with more recent data, as I think any additional functionality beyond what I've done to finish this deliverable would be a colossal waste of time using this data.

Overall, I learned a ton in working on this assignment and feel much more confident in my scraping abilities for future (more helpful!) datasets on the internet than I did before. :-)

### April 1 Update

I'm still not as far as I would like to be, but I'm very proud of the progress I made on my bot this week in terms of breaking down different problems and things that I can do with my data. For this week, I pivoted from trying to scrape the Maryland state employees webpages by agency/sub-agency/sub-sub-agency and instead began working to pull from the pages that have each employee listed individually with information. The structure of these pages (faceted by letter with multiple pages per letter) is currently the big road block I am facing (pagination_fixed, chat_gpt_workshop and scrape.py). I'm struggling now to get the code to iterate through each letter first and then iterate over the specific pages associated with that letter as a sub-loop. I'm trying to make the iteration happen now by parsing through the URL in increments of 15 (as each page on the site ends in a variable number that is incremented by 15 to get to the next one. Each page starts with that number as zero, and then the next page moves to offset=15, offset=30 and so on until all of the pages have been found). I asked ChatGPT for help a lot during this troubleshooting process, but it keeps trying to find the next page *links* through parsing <a> tages. I haven't been able to reroute its advice to help out in a useful way that iterates through values in the URL yet.

With that said, I started working on a scraper that scraped into different lists for each letter where there are state employees (see scrape_by_letter). I was able to get all of the "A" values pulled down/etc. but am planning Sunday to troubleshoot why some letters get pulled using this (repetitive, but last resort) strategy, so the code may change a bit between now and when Derek sees this. I think I'll try to store the individual letter scrapes within functions and find a way to add them to a big list of every employee based on that (which will be the one that is monitored and written to the CSV in the final bot process), as things seem to be getting lost in that big scraping shuffle now. 
    (As an aside, the main pitfall with relying on this strategy is the fact that I have hard-coded how many pages/multiples of 15 for each letter should be present for the scraper to continue working. This is far from my favorite, but I figured as an absolute last resort to get *all* of the data down, checking up on the number of pages/seeing if jobs have been added every so often (therefore throwing off the number of pages designated in each specific letter) is still much better than scrolling through thousands of pages consistently trying to pull specific rows.)

The working part of my bot (and the one that is currently set up in the YAML files) is one that just pulls the vacancies from the state employee database. I plan to spend the rest of Saturday figuring out a way to structure this data, as right now it writes to a CSV but I need a refresher on what the most useful way for me to aggregate it within the bot would be. I plan to have some sort of data layout with "[x] vacancies have been filled this week. there are [x] vacancies remaining." When all is said and done, the bot will scrape the vacancies page once a week, save that version as a CSV file with the date when it was scraped and then push a Slack message about the vacancies based on aggegating the numbers/types of vacancies that were filled and that still remain. That will be the barebones bot that will result from this project if I have truly hit a wall with getting every single employee down. At the very least, I am 100% confident I will be able to get this working. I feel alright about being able to get the full scrape of all employees functional, as I at least have a good semblance of how to do it (even if it isn't in the most efficient way yet).

My main objectives for this week are setting up the actual functionality of the bot and making sure that it is well-integrated into its Slack form. But I think this past week was a huge step forward, as last time we had a deadline I couldn't even really figure out the best way to pull the data down. Now, I have a bunch of templates for how to moderate the information depending on what I'm able to troubleshoot over the next few days. My main goal for Sunday/Monday/Tuesday is taking more jabs at having a holistic scraper for all of the employees with some help from Derek/ChatGPT/letting the problem sit for a minute (fingers crossed for a breakthrough!) and then putting some time and energy into making sure the Slackbot is working when it is supposed to/fully integrated in terms of how the message is laid out. 

### Pre-Spring Break Update

A disclaimer up top: I wanted to be way further into understanding how to get the data through deducing the problem to a single agency (in my case here, the state education department), but am headed out of town tonight (Saturday) and wanted to have this done before spring break officially began. I plan to keep toodling with things during the week as time permits.

For this assignment, my main goal was understanding the code of Derek's original scraper for this site, what needs to be changed for my specific purposes of this data that I'm pulling and trying to understand how to scrape the specifics that I need using a particular, single agency that I could scrape.

If nothing else, I think I have a much better understanding of this data and its information architecture through working through different parts of Derek's initial code and working to update/replicate things for just the state education department. What I'm mainly stuck on right now is getting just the

Something I didn't get around to this week (because man, pre-spring break work really seems to pile up) was updating portions of Derek's original scraper that pull down positions that aren't actually real. I think it has something to do with the provisions of the code where we are looking for contact information, such as lines like:
    (if row.find_all('td')[1].text.strip().upper() in ['GENERAL LISTING', 'TTY', 'TOLL FREE', 'MARYLAND TOLL FREE', 'GENERAL ASSISTANCE', 'FAX NUMBER', 'INFORMATION', 'TOLL FREE NUMBER', 'FACSIMILE', 'FACSIMILE NUMBER', 'FASCIMILE NUMBER', 'MAIN NUMBERS', 'MAIN NUMBER', 'GENERAL INFORMATION', 'GENERAL ASSEMBLY INFORMATION']:
                continue)

Those were the main instances where I could see things pulling down positions and pages that weren't actually home to employee information, as things like the "general information", "facsmile number" and other contact categories aren't typically routing to individual positions/jobs/people within the website's data, but instead a general toll number line/something similar to this. This is tricky because the information that doesn't have employee names/etc. in the table is structured the same way on each page as information about given vacancies, so I'm working to find solutions to how we can pull only employee information alone or how to distinguish that based on the HTML that we have to work with.

A main thing I was trying to accomplish this week (and didn't quite achieve) was getting sub-sub-agencies from the data. The scraper Derek had only pulled the agency information and subagency information, but many state employers also have additional pages associated with each subagency. For example, with our education department pages that I was workshopping and troubleshooting this week, there is:
    - A page for all the general agencies in the education department: https://www.doit.state.md.us/phonebook/level2offices.asp?AID=MSDE&offset=0
    - A page for each individual agency (in this example, the superintendant's office) that has employee information: https://www.doit.state.md.us/phonebook/OfficeSub.asp?OID=393
    - A page for each sub-sub agency within the superintendent's office and each other sub-agency that isn't currently getting scraped (in this example, the office of legal counsel from the state superintendent's office): https://www.doit.state.md.us/phonebook/OfficeSub.asp?OID=394

I'm having a hard time reducing these different pages to pull from into their most basic parts without hard-coding things into the scraper that won't be repurposable for other departments in the data. I frankly think I'm dealing with a fair amount of data overload just from the sheer number of pages of data to pull from and have hit a bit of a wall with what I've been trying to accomplish in the waning hours before spring break begins. 

There are also some offices that have links pulled down (such as the division of business services: https://www.doit.state.md.us/phonebook/OfficeSub.asp?OID=8066) that don't list any base employees on the page but do link to sub-agencies that have employees we will need to pull down. On a similar note, there are pages that don't link to any employee information whatsoever.

Something I'm wondering about in terms of troubleshooting/not making this scraper extremely convoluted is if there's a way to scrape from the searches for particularly vacant positions (seen here: https://www.doit.state.md.us/phonebook/IndListing.asp?FirstLetter=vacant&Submit=Search&offset=0) and seeing when things disappear or reappear on that page. Shreya had her scraper do something where it searched stuff in a search bar, so I don't know if that would be an easier approach to this task. Even scraping from the general search bar that has every single employee, their office and department in one place (i.e. https://www.doit.state.md.us/phonebook/AdvSearchResults.asp) seems a bit more feasible than dipping in and out of so many pages within the scraper as it works now/as I had outlined it in my head. I definitely plan to toy around with these possibilities over break and early in the week we come back to see if they're easier to work with, but like I said, I just wanted to get this assignment done before heading out for the week, so I'll leave my rambling about problems and troubleshooting at this. :-)

When it's finished, this bot should run in Slack, ideally, but I'm open to options for whatever will make it work best. The best version of this bot gives alerts each time a state employee vacany has been filled and is able to provide swift and timely alerts when that happens. In a perfect world, it should also able to count up how many vacancies there are, how many have been filled and do basic arithmetic so that people who see the bot can understand any slight trends in how state employee is hiring (based solely on how many positions have been filled/not filled since x date). 